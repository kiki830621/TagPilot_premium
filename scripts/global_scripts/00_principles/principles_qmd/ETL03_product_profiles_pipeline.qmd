---
id: "ETL003"
title: "ETL03 product Profiles Pipeline (0IM→1ST→2TR)"
type: "etl-operations"
date_created: "2025-07-12"
date_modified: "2025-07-15"
author: "Claude"
derives_from:
  - "DF000": "Data Pipeline Architecture - 7-Layer"
  - "DF000": "Data Pipeline Architecture"
implements:
  - "ETL03": "Amazon product Profiles Pipeline"
relates_to:
  - "MP0052": "Unidirectional Data Flow"
  - "R091": "Universal Data Access"
  - "R092": "Universal DBI Approach"
---

# ETL03: Amazon product Profiles Pipeline (0IM→1ST→2TR)

## Core Purpose

ETL03 implements a **three-phase pipeline** for processing Amazon product profiles from Google Sheets through staged validation to transformed business-ready data. The pipeline follows the 7-layer ETL architecture focusing on Import (0IM), Staging (1ST), and Transform (2TR) phases.

## Pipeline Overview

```mermaid
graph TD
    A[Google Sheets API] --> B[raw_data.duckdb]
    B --> C[staged_data.duckdb] 
    C --> D[transformed_data.duckdb]
    
    A -.-> A1["ETL03_0IM<br/>• Google Sheets import<br/>• Product line detection<br/>• Raw data storage"]
    B -.-> B1["ETL03_1ST<br/>• Data validation<br/>• Type detection<br/>• UTF-8 conversion"]
    C -.-> C1["ETL03_2TR<br/>• Column mapping<br/>• Standardization<br/>• Business format"]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
```

**Current Implementation**: Three-phase pipeline with focused processing  
**Input**: Google Sheets with Amazon product profiles by product line  
**Output**: Standardized product profiles in `transformed_data.duckdb`

## ETL03 vs ETL04 Separation

**ETL03** focuses on **product profiles** (product characteristics, ratings, prices)  
**ETL04** focuses on **competitor analysis** (competitor products, positioning data)

| Pipeline | Purpose | Input Data | Output Tables |
|----------|---------|------------|---------------|
| ETL03 | product Profiles | Google Sheets product profile data | `df_product_profile_{product_line}___transformed` |
| ETL04 | Competitor Analysis | Google Sheets competitor data | `df_amz_competitor_product_id___transformed` |

**Migration Note**: Competitor products import (formerly `amz_ETL03_0IM_01.R`) has been moved to ETL04 pipeline as `amz_ETL04_0IM_legacy.R` and reimplemented as the new ETL04 three-phase pipeline.

## ETL03 Script Implementation

### Script Sequence Overview

The ETL03 pipeline consists of three sequential scripts:

1. **`amz_ETL03_0IM.R`** - Import Phase (0IM) - Main product profiles
2. **`amz_ETL03_0IM_review_properties.R`** - Import Phase (0IM) - Review properties
3. **`amz_ETL03_0IM_product_reviews.R`** - Import Phase (0IM) - Product reviews
4. **`amz_ETL03_1ST.R`** - Staging Phase (1ST)  
5. **`amz_ETL03_2TR.R`** - Transform Phase (2TR)

### Configuration Structure

```yaml
# Configuration via app_configs structure
list_colname_aliases:
  standard_names:
    product_brand:
      - "品牌"
      - "pin_pai"
      - "brand"
    product_id:
      - "asin"
      - "product_code"
    product_title:
      - "產品名稱"
      - "商品名稱"
      - "title"
    price:
      - "價格"
      - "price"
    rating:
      - "顧客評分"
      - "rating"
    num_rating:
      - "顧客評分則數"
      - "num_rating"
```

```r
# Database paths configuration
db_path_list <- list(
  raw_data = "data/local_data/raw_data.duckdb",
  staged_data = "data/local_data/staged_data.duckdb",
  transformed_data = "data/local_data/transformed_data.duckdb"
)

# Google Sheets configuration
google_sheet_config <- list(
  sheet_id = "16-k48xxFzSZm2p8j9SZf4V041fldcYnR8ectjsjuxZQ",
  sheet_prefix = "product_profile_"
)
```

## Script Implementations

### ETL03_0IM.R - Import Phase

```r
# amz_ETL03_0IM.R - Import Amazon product Profiles
# Import (0IM): Google Sheets → raw_data.duckdb

# Initialize environment
autoinit()

# Main execution
message("0IM: Starting Amazon product profiles import...")

# Connect to databases
raw_data <- dbConnectDuckdb(db_path_list$raw_data, read_only = FALSE)

# Load product line configuration
product_lines <- dbGetQuery(app_data, "SELECT * FROM df_product_line WHERE is_active = 1")

# Import each product line
for (i in 1:nrow(product_lines)) {
  product_line <- product_lines[i, ]
  sheet_name <- paste0("product_profile_", product_line$product_line_id)
  
  tryCatch({
    # Import from Google Sheets
    import_result <- import_product_profiles(
      sheet_id = google_sheet_config$sheet_id,
      sheet_name = sheet_name,
      product_line_info = product_line,
      target_connection = raw_data
    )
    
    message("0IM: Imported ", import_result$rows_imported, 
            " products for product line: ", product_line$product_line_id)
    
  }, error = function(e) {
    warning("0IM: Failed to import ", sheet_name, ": ", e$message)
  })
}

message("0IM: Amazon product profiles import completed")
autodeinit()

### ETL03_1ST.R - Staging Phase

```r
# amz_ETL03_1ST.R - Stage Amazon product Profiles  
# Staging (1ST): raw_data.duckdb → staged_data.duckdb

# Initialize environment
autoinit()

# Main execution
message("1ST: Starting Amazon product profiles staging...")

# Connect to databases
raw_data <- dbConnectDuckdb(db_path_list$raw_data, read_only = TRUE)
staged_data <- dbConnectDuckdb(db_path_list$staged_data, read_only = FALSE)

# Stage all product profile tables
stage_results <- stage_product_profiles(
  raw_db_connection = raw_data,
  staged_db_connection = staged_data,
  table_pattern = "^df_product_profile_.*$",
  overwrite_existing = TRUE
)

message("1ST: Staged ", stage_results$total_rows_staged, 
        " product profiles across ", length(stage_results$tables_processed), " tables")

# Verify staging results
if (stage_results$total_rows_staged > 0) {
  message("1ST: Amazon product profiles staging completed successfully")
} else {
  warning("1ST: No product profiles were staged")
}

autodeinit()

### ETL03_2TR.R - Transform Phase

```r
# amz_ETL03_2TR.R - Transform Amazon product Profiles
# Transform (2TR): staged_data.duckdb → transformed_data.duckdb

# Initialize environment
autoinit()

# Main execution
message("2TR: Starting Amazon product profiles transformation...")

# Connect to databases
staged_data <- dbConnectDuckdb(db_path_list$staged_data, read_only = TRUE)
transformed_data <- dbConnectDuckdb(db_path_list$transformed_data, read_only = FALSE)

# Load column aliases configuration
if (exists("app_configs") && "list_colname_aliases" %in% names(app_configs)) {
  message("2TR: Column aliases loaded for: ", 
          paste(names(app_configs$list_colname_aliases$standard_names), collapse = ", "))
} else {
  warning("2TR: Column aliases configuration not found")
}

# Transform all staged product profile tables
transform_results <- transform_product_profiles(
  staged_db_connection = staged_data,
  transformed_db_connection = transformed_data,
  table_pattern = "^df_product_profile_.*___staged$",
  overwrite_existing = TRUE
)

message("2TR: Transformed ", transform_results$total_rows_transformed, 
        " product profiles across ", length(transform_results$tables_processed), " tables")

# Verify transformation results
if (transform_results$total_rows_transformed > 0) {
  message("2TR: Amazon product profiles transformation completed successfully")
} else {
  warning("2TR: No product profiles were transformed")
}

autodeinit()

```

## Core Functions

### Import Functions

#### import_product_profiles()

```r
#' Import product Profiles from Google Sheets
#'
#' @param sheet_id Google Sheets ID
#' @param sheet_name Name of the sheet to import
#' @param product_line_info Product line metadata
#' @param target_connection DBI connection to raw database
#' @return List with import results
import_product_profiles <- function(sheet_id, sheet_name, product_line_info, target_connection) {
  
  # Read from Google Sheets
  sheet_data <- googlesheets4::read_sheet(sheet_id, sheet = sheet_name)
  
  # Add metadata
  sheet_data$etl_import_timestamp <- Sys.time()
  sheet_data$etl_import_source <- sheet_name
  sheet_data$etl_product_line_name <- product_line_info$product_line_name
  sheet_data$etl_product_line_id <- product_line_info$product_line_id
  
  # Generate table name
  table_name <- paste0("df_product_profile_", product_line_info$product_line_id)
  
  # Store in raw database
  dbWriteTable(target_connection, table_name, sheet_data, 
               overwrite = TRUE, append = FALSE)
  
  return(list(
    table_name = table_name,
    rows_imported = nrow(sheet_data),
    columns_imported = ncol(sheet_data)
  ))
}
```

### Staging Functions

#### stage_product_profiles()

```r
#' Stage product Profiles with Validation and Type Detection
#'
#' @param raw_db_connection DBI connection to raw database
#' @param staged_db_connection DBI connection to staged database
#' @param table_pattern Regex pattern to match raw tables
#' @param overwrite_existing Whether to overwrite existing staged tables
#' @return List with staging results
stage_product_profiles <- function(raw_db_connection, staged_db_connection, 
                               table_pattern = "^df_product_profile_.*$", 
                               overwrite_existing = TRUE) {
  
  # Get matching raw tables
  raw_tables <- DBI::dbListTables(raw_db_connection)
  product_profile_tables <- raw_tables[grepl(table_pattern, raw_tables)]
  
  staging_results <- list(
    tables_processed = character(),
    total_rows_staged = 0,
    processing_details = list()
  )
  
  for (table_name in product_profile_tables) {
    
    # Load raw data
    raw_data_df <- DBI::dbGetQuery(raw_db_connection, 
                                  paste("SELECT * FROM", table_name))
    
    # Apply data type detection and optimization
    optimized_data <- detect_and_optimize_types(raw_data_df)
    
    # Add staging metadata
    optimized_data$etl_phase <- "stage"
    optimized_data$etl_staging_timestamp <- Sys.time()
    optimized_data$etl_validation_status <- "passed"
    
    # Generate staged table name
    staged_table_name <- paste0(table_name, "___staged")
    
    # Write to staged database using optimized schema
    write_with_optimized_schema(staged_db_connection, staged_table_name, 
                               optimized_data, overwrite_existing)
    
    # Update results
    staging_results$tables_processed <- c(staging_results$tables_processed, 
                                         staged_table_name)
    staging_results$total_rows_staged <- staging_results$total_rows_staged + nrow(optimized_data)
    staging_results$processing_details[[staged_table_name]] <- list(
      source_table = table_name,
      rows_staged = nrow(optimized_data),
      columns_staged = ncol(optimized_data)
    )
  }
  
  return(staging_results)
}
```

### Transform Functions

#### transform_product_profiles()

```r
#' Transform product Profiles to Business-Ready Format
#'
#' @param staged_db_connection DBI connection to staged database
#' @param transformed_db_connection DBI connection to transformed database
#' @param table_pattern Regex pattern to match staged tables
#' @param overwrite_existing Whether to overwrite existing transformed tables
#' @return List with transformation results
transform_product_profiles <- function(staged_db_connection, transformed_db_connection,
                                   table_pattern = "^df_product_profile_.*___staged$",
                                   overwrite_existing = TRUE) {
  
  # Get matching staged tables
  staged_tables <- DBI::dbListTables(staged_db_connection)
  product_profile_tables <- staged_tables[grepl(table_pattern, staged_tables)]
  
  transform_results <- list(
    tables_processed = character(),
    total_rows_transformed = 0,
    processing_details = list()
  )
  
  for (table_name in product_profile_tables) {
    
    # Load staged data
    staged_data_df <- DBI::dbGetQuery(staged_db_connection, 
                                     paste("SELECT * FROM", table_name))
    
    # Extract product line ID
    product_line_match <- regmatches(table_name, 
                                   regexpr("df_product_profile_([^_]+)", table_name))
    product_line_id <- gsub("df_product_profile_", "", product_line_match)
    
    # Standardize basic fields using aliases
    transformed_df <- standardize_basic_fields(staged_data_df, product_line_id)
    
    # Add transformation metadata
    transformed_df$etl_transform_timestamp <- Sys.time()
    transformed_df$etl_phase <- "transform"
    transformed_df$schema_version <- "2TR_v2.1_direct_mapping"
    
    # Generate transformed table name
    transformed_table_name <- gsub("___staged$", "___transformed", table_name)
    
    # Write to transformed database
    DBI::dbWriteTable(transformed_db_connection, transformed_table_name, 
                     transformed_df, overwrite = overwrite_existing, append = FALSE)
    
    # Update results
    transform_results$tables_processed <- c(transform_results$tables_processed, 
                                          transformed_table_name)
    transform_results$total_rows_transformed <- transform_results$total_rows_transformed + nrow(transformed_df)
    transform_results$processing_details[[transformed_table_name]] <- list(
      source_table = table_name,
      product_line_id = product_line_id,
      rows_transformed = nrow(transformed_df),
      columns_transformed = ncol(transformed_df)
    )
  }
  
  return(transform_results)
}
```

## Execution Workflow

### Sequential Script Execution

The ETL03 pipeline should be executed in order:

```bash
# Execute the complete ETL03 pipeline
Rscript scripts/update_scripts/amz_ETL03_0IM.R
Rscript scripts/update_scripts/amz_ETL03_0IM_review_properties.R
Rscript scripts/update_scripts/amz_ETL03_0IM_product_reviews.R
Rscript scripts/update_scripts/amz_ETL03_1ST.R  
Rscript scripts/update_scripts/amz_ETL03_2TR.R

# Export results to CSV for analysis
Rscript scripts/update_scripts/all_S02_00.R
```

### Individual Script Execution

```bash
# Import only (0IM phase)
Rscript scripts/update_scripts/amz_ETL03_0IM.R
Rscript scripts/update_scripts/amz_ETL03_0IM_review_properties.R
Rscript scripts/update_scripts/amz_ETL03_0IM_product_reviews.R

# Stage only (1ST phase) - requires 0IM to be completed
Rscript scripts/update_scripts/amz_ETL03_1ST.R

# Transform only (2TR phase) - requires 1ST to be completed  
Rscript scripts/update_scripts/amz_ETL03_2TR.R
```

## Key Features of ETL03 Implementation

### 1. Three-Phase Architecture
- **0IM (Import)**: Google Sheets → raw_data.duckdb with metadata preservation
- **1ST (Staging)**: Type detection, validation, and optimization for DuckDB
- **2TR (Transform)**: Column mapping and business format standardization

### 2. Configuration-Driven Column Mapping
- YAML-based column aliases in `app_configs$list_colname_aliases`
- Flexible mapping from Chinese/English source columns to standard names
- Support for one-to-many alias relationships

### 3. Data Type Optimization
- Automatic detection of optimal DuckDB types
- Schema generation with `generate_create_table_query`
- Efficient storage and query performance

### 4. Robust Error Handling
- Comprehensive validation at each phase
- Graceful handling of missing sheets and malformed data
- Detailed logging and progress reporting

### 5. Product Line Support
- Multi-product line processing (jew, sop, etc.)
- Consistent table naming: `df_product_profile_{product_line}___{phase}`
- Metadata tracking for lineage and debugging

### 6. Integration with 7-Layer Architecture
- Follows unidirectional data flow (MP0052)
- Compatible with downstream cleansing and processing phases
- Ready for component consumption patterns

This ETL03 implementation provides a solid foundation for Amazon product profile processing while maintaining flexibility for future product lines and data sources.

## Data Flow Summary

| Phase | Input | Output | Key Operations |
|-------|-------|--------|--------------|
| 0IM | Google Sheets | raw_data.duckdb | Import, metadata addition |
| 1ST | raw_data.duckdb | staged_data.duckdb | Validation, type detection, UTF-8 |
| 2TR | staged_data.duckdb | transformed_data.duckdb | Column mapping, standardization |

### Current Processing Statistics

- **Product Lines**: jew (178 products), sop (192 products)
- **Total products**: 370 product profiles processed
- **Schema Version**: 2TR_v2.1_direct_mapping
- **Column Aliases**: 6 standard fields mapped (product_brand, product_id, product_title, price, rating, num_rating)

The ETL03 pipeline successfully processes Amazon product profiles with robust error handling, flexible configuration, and optimized data structures for downstream consumption.