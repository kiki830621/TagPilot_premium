---
id: "DF000"
title: "Data Pipeline Architecture"
type: "data-flow"
date_created: "2025-07-12"
date_modified: "2025-07-12"
author: "Claude"
derives_from:
  - "MP0052": "Unidirectional Data Flow"
  - "MP0006": "Data Source Hierarchy"
  - "MP0058": "Database Table Creation Strategy"
influences:
  - "DF001": "Staging Operations"
  - "DF002": "Transformation Operations"
  - "DF003": "Cleansing Operations"
  - "DF004": "Processing Operations"
  - "DF005": "Application Data"
relates_to:
  - "MP0018": "Don't Repeat Yourself"
  - "R091": "Universal Data Access"
  - "R092": "Universal DBI Approach"
---

# DF00: Data Pipeline Architecture

## Core Principle

The data pipeline follows a five-layer unidirectional architecture that ensures data integrity, traceability, and maintainability through explicit separation of concerns at each processing stage.

## Five-Layer Architecture Overview

```mermaid
graph LR
    A[raw_data] --> B[staged_data]
    B --> C[transformed_data]
    C --> D[cleansed_data]
    D --> E[processed_data]
    E --> F[app_data]
    
    A -.-> A1["• 外部來源原檔<br/>• 0% 變動<br/>• 完全保真"]
    B -.-> B1["• 輕量封裝<br/>• 編碼統一<br/>• 格式標準化"]
    C -.-> C1["• 欄位/型別統一<br/>• Schema標準化<br/>• Reference Join"]
    D -.-> D1["• 資料品質處理<br/>• 去重/驗證<br/>• 缺失值處理"]
    E -.-> E1["• 業務邏輯聚合<br/>• KPI計算<br/>• Star Schema"]
    F -.-> F1["• 應用層資料<br/>• UI消費就緒<br/>• 最終產品"]
```

## Layer Definitions

### Layer 1: Raw Data
- **Purpose**: 原始資料保存層
- **Database**: `raw_data.duckdb`
- **Directory**: `00_raw/`
- **Operations**: Import only, no modifications
- **Characteristics**:
  - 100% faithfully preserves external source data
  - No encoding changes, no content modifications
  - Acts as immutable source of truth
  - May contain mixed encodings, formats, compression

### Layer 2: Staged Data  
- **Purpose**: 檔案層級標準化
- **Database**: `staged_data.duckdb` 
- **Directory**: `05_stage/` or `10_ingested/`
- **Operations**: File-level preprocessing only
- **Characteristics**:
  - Unified encoding (UTF-8, BOM removal)
  - Decompression and archive extraction  
  - Line ending standardization (CRLF → LF)
  - Metadata logging (file size, hash, row count)
  - **No content values modified**

### Layer 3: Transformed Data
- **Purpose**: Schema層級標準化  
- **Database**: `transformed_data.duckdb`
- **Directory**: `20_transformed/`
- **Operations**: Column and type standardization
- **Characteristics**:
  - Column renaming and reordering
  - Data type conversions
  - Foreign key lookups and joins
  - Simple derived fields
  - Reference data integration

### Layer 4: Cleansed Data
- **Purpose**: 資料品質保證
- **Database**: `cleansed_data.duckdb`
- **Directory**: `30_cleansed/`  
- **Operations**: Quality assurance and validation
- **Characteristics**:
  - Deduplication logic
  - Missing value handling
  - Outlier detection and treatment
  - Format validation
  - Business rule validation

### Layer 5: Processed Data
- **Purpose**: 業務邏輯處理
- **Database**: `processed_data.duckdb`
- **Directory**: `40_processed/`
- **Operations**: Business aggregation and calculation
- **Characteristics**:
  - Customer segmentation
  - RFM calculations
  - KPI aggregations
  - Star/Snowflake schema creation
  - Complex business metrics

### Layer 6: Application Data
- **Purpose**: 應用消費層
- **Database**: `app_data.duckdb`
- **Directory**: `app_data/`
- **Operations**: Application-optimized data preparation
- **Characteristics**:
  - UI-ready data structures
  - Application-specific views
  - Performance-optimized tables
  - Final consumption format

## Implementation Principles

### 1. Unidirectional Flow ([@MP0052])

Data flows in one direction only:
```
raw_data → staged_data → transformed_data → cleansed_data → processed_data → app_data
```

**No backwards dependencies**: Each layer depends only on previous layers, never on subsequent ones.

### 2. Immutability Principle

- **Raw Data**: Never modified after import
- **Staged Data**: Only file-level changes, content remains identical  
- **Transformed+**: Each layer creates new data rather than modifying source

### 3. Single Responsibility per Layer

Each layer has exactly one responsibility:

| Layer | Responsibility | What It Does | What It Doesn't Do |
|-------|---------------|--------------|-------------------|
| Raw | Import | Store original files | Format, validate, transform |
| Staged | File Prep | Encoding, compression | Content changes, schema changes |
| Transformed | Schema | Columns, types, structure | Quality, business logic |
| Cleansed | Quality | Validation, deduplication | Aggregation, metrics |
| Processed | Business | KPIs, segmentation | UI formatting |
| Application | Consumption | UI-ready data | Business calculations |

### 4. Error Isolation

Errors are contained within layers:
- **Raw layer failure**: Re-download source data
- **Staging failure**: Fix encoding/format issues, re-run staging
- **Transform failure**: Fix schema mapping, re-run from staged
- **Cleansing failure**: Fix quality rules, re-run from transformed
- **Processing failure**: Fix business logic, re-run from cleansed

## Database Architecture

### Database File Organization

```r
# Database paths following fn_get_default_db_paths.R pattern
list(
  raw_data = file.path(COMPANY_DIR, "raw_data.duckdb"),
  staged_data = file.path(COMPANY_DIR, "staged_data.duckdb"),           # NEW
  transformed_data = file.path(COMPANY_DIR, "transformed_data.duckdb"), # NEW
  cleansed_data = file.path(COMPANY_DIR, "cleansed_data.duckdb"),
  processed_data = file.path(COMPANY_DIR, "processed_data.duckdb"),
  app_data = file.path(APP_DATA_DIR, "app_data.duckdb")
)
```

### Universal Access Pattern ([@R091], [@R092])

All database access follows the universal `tbl2()` pattern:

```r
# Load database utility functions
source("scripts/global_scripts/02_db_utils/fn_tbl2.R")

# Access data at any layer
raw_customers <- tbl2(dbConnect_from_list("raw_data"), "customers")
staged_customers <- tbl2(dbConnect_from_list("staged_data"), "customers")  
transformed_customers <- tbl2(dbConnect_from_list("transformed_data"), "customers")
# ... etc
```

## Staging Operations Detail

### Why Staging is Essential

Staging solves critical file-level issues that cause downstream failures:

1. **Encoding Problems**: Mixed UTF-8/Latin1/UTF-16 files
2. **BOM Issues**: Byte-order marks breaking CSV parsing
3. **Line Ending Chaos**: Windows CRLF vs Unix LF  
4. **Compression Handling**: Mixed zip/gz/tar archives
5. **Metadata Loss**: No audit trail of file changes

### Staging Example Implementation

```r
# staging.R - File-level preprocessing
library(fs)
library(readr)
library(jsonlite)

stage_files <- function(source_dir = "00_raw", target_dir = "05_stage") {
  dir_create(target_dir)
  
  raw_files <- dir_ls(source_dir, glob = "*.csv")
  
  for (f in raw_files) {
    message("Staging: ", f)
    
    # Read with encoding detection and BOM removal
    txt <- read_lines(f, locale = locale(encoding = "UTF-8-BOM"))
    
    # Standardize line endings and write
    new_path <- path(target_dir, path_file(f))
    write_lines(txt, new_path, sep = "\n")
    
    # Log metadata for audit trail
    log_entry <- list(
      file = path_file(f),
      stage_time = Sys.time(),
      bytes = file_size(new_path),
      hash = digest::digest(file = new_path, algo = "xxhash64"),
      rows = length(txt)
    )
    
    write_lines(
      toJSON(log_entry, auto_unbox = TRUE),
      file.path(target_dir, "stage_log.jsonl"),
      append = TRUE
    )
  }
}
```

## Benefits of Five-Layer Architecture

### 1. Clear Separation of Concerns
Each layer has a single, well-defined responsibility, making the system easier to understand and maintain.

### 2. Enhanced Debugging
Issues can be isolated to specific layers:
- File corruption → Raw/Staged layers
- Schema mismatches → Transformed layer  
- Data quality issues → Cleansed layer
- Business logic bugs → Processed layer
- UI problems → Application layer

### 3. Incremental Processing
Failed processes can restart from the appropriate layer without re-running entire pipeline.

### 4. Audit Trail
Complete data lineage from source to consumption with metadata logging at each stage.

### 5. Reusability
Intermediate layers can serve multiple downstream consumers:
- Cleansed data → Multiple processing pipelines
- Processed data → Multiple applications

## Migration from Current Architecture

### Current State
```
raw_data → cleansed_data → processed_data → app_data
```

### Migration Path
1. **Add staging layer** between raw and cleansed
2. **Add transformation layer** between staging and cleansed  
3. **Update database paths** in `fn_get_default_db_paths.R`
4. **Refactor D01/D02** to use new layer operations
5. **Update initialization scripts** in `db_setup.R`

### Backwards Compatibility
Existing code continues to work:
- `cleansed_data.duckdb` remains available
- Current tbl2() calls continue functioning
- Migration can be gradual, layer by layer

## Relationship to Existing Principles

### Derives From
- **[@MP0052]**: Provides unidirectional flow foundation
- **[@MP0006]**: Extends data source hierarchy with clear layers
- **[@MP0058]**: Applies table creation strategies per layer type

### Supports  
- **[@MP0018]**: Eliminates code duplication by standardizing operations
- **[@R091]**, **[@R092]**: Universal data access across all layers

### Enables
- **ETL Series**: DF01-DF05 implement specific layer operations
- **D Series Refactoring**: D01, D02 can reference standardized ETL operations

## Conclusion

The five-layer data pipeline architecture provides a robust, maintainable, and scalable foundation for data processing. By clearly separating file-level, schema-level, quality-level, and business-level concerns, it enables better debugging, clearer code organization, and more reliable data processing workflows.

Each layer serves a specific purpose and can be developed, tested, and maintained independently while ensuring overall system integrity through unidirectional data flow.